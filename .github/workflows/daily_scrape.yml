name: Daily Air Quality Scrape

on:
  schedule:
    # Runs at 08:00 UTC every day. Change this if you want different times.
    # To run every hour, use: '0 * * * *'
    - cron: '29 * * * *'
  workflow_dispatch: # Allows you to click a button to run it manually for testing

permissions:
  contents: write

jobs:
  scrape-and-commit:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Scraper
        run: python scrape.py

      # Ensure you still have fetch-depth: 0 in your checkout step!

      - name: Run Scraper
        run: python scrape.py

      # ðŸš¨ DELETE the old 'Commit and Push Data' step and REPLACE with this action ðŸš¨
      - name: Commit and Push Data (Using Auto-Commit Action)
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          # Which files to commit. This targets only your CSV file.
          file_pattern: air_quality_data.csv
          # Set the commit message
          commit_message: "Auto-update air quality data"
          # Optional: Use a specific bot email for cleaner history
          commit_user_name: GitHub Action Scraper
          commit_user_email: action@github.com
          
          # This action handles the authentication and push safely.
